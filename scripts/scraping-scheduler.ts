#!/usr/bin/env tsx
import { loggers } from '@/lib/logger'
const log = loggers.enrichment

/**
 * Script de planification du scraping
 * Ã€ exÃ©cuter toutes les heures via cron job
 */

import { globalScraper } from '../services/scraping/data-scraper'
import { prisma } from '../lib/db'

async function main() {
  log.info('ğŸ• Scraping Scheduler - DÃ©marrage...\n')

  try {
    // 1. RÃ©cupÃ©rer les nouveaux devis sans scraping
    const recentDevis = await prisma.devis.findMany({
      where: {
        createdAt: {
          gte: new Date(Date.now() - 1000 * 60 * 60 * 24), // DerniÃ¨res 24h
        },
      },
      take: 50,
      orderBy: { createdAt: 'desc' },
    })

    log.info(`ğŸ“‹ ${recentDevis.length} devis rÃ©cents trouvÃ©s\n`)

    // 2. Programmer le scraping pour chaque devis
    for (const devis of recentDevis) {
      await globalScraper.scheduleDevisScraping(devis.id)
    }

    // 3. Traiter la queue
    log.info('\nğŸš€ Traitement de la queue...\n')
    await globalScraper.processQueue()

    // 4. Afficher les statistiques
    const stats = globalScraper.getQueueStats()
    log.info('\nğŸ“Š Statistiques:')
    log.info(`  Total: ${stats.total}`)
    log.info(`  En attente: ${stats.pending}`)
    log.info(`  En cours: ${stats.inProgress}`)
    log.info(`  ComplÃ©tÃ©es: ${stats.completed}`)
    log.info(`  Ã‰chouÃ©es: ${stats.failed}\n`)

    log.info('âœ… Scraping scheduler terminÃ©\n')

  } catch (error) {
    log.error('âŒ Erreur:', error)
    process.exit(1)
  } finally {
    await prisma.$disconnect()
  }
}

main()

