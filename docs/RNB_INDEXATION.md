# ğŸ—ï¸ SystÃ¨me d'Indexation Progressive RNB

Ce document dÃ©crit le systÃ¨me d'indexation progressive des donnÃ©es RNB (RÃ©fÃ©rentiel National des BÃ¢timents) dans TORP.

## ğŸ“‹ Vue d'ensemble

Les fichiers RNB sont volumineux (50-200MB par dÃ©partement). Le systÃ¨me permet de :
- **Indexer progressivement** les fichiers CSV par dÃ©partement
- **Stocker localement** dans PostgreSQL pour recherche rapide
- **Importer Ã  la demande** des bÃ¢timents spÃ©cifiques
- **Suivre la progression** des imports avec jobs

## ğŸ›ï¸ Architecture

### 1. **RNBService** (`services/external-apis/rnb-service.ts`)
- RÃ©cupÃ¨re les mÃ©tadonnÃ©es depuis data.gouv.fr
- Identifie les ressources par dÃ©partement
- Appelle l'indexeur si donnÃ©es disponibles localement

### 2. **RNBIndexer** (`services/external-apis/rnb-indexer.ts`)
- Recherche dans la base de donnÃ©es locale
- Indexe les bÃ¢timents par batch
- GÃ¨re les jobs d'import
- Statistiques par dÃ©partement

### 3. **RNBImporter** (`services/external-apis/rnb-importer.ts`)
- TÃ©lÃ©charge et parse les fichiers CSV ZIP
- Importe progressivement par batch
- GÃ¨re les callbacks de progression

## ğŸ’¾ ModÃ¨les de Base de DonnÃ©es

### `RNBBuilding`
Stocke les donnÃ©es des bÃ¢timents indexÃ©s :
- Identifiant RNB original
- DÃ©partement, code INSEE, commune
- CoordonnÃ©es GPS
- DonnÃ©es bÃ¢ti (annÃ©e, type, surface)
- DonnÃ©es DPE (classe, consommation, Ã©missions)
- HVD (Haute Valeur DÃ©terminante)

### `RNBImportJob`
Suit les jobs d'import par dÃ©partement :
- Statut (PENDING, IN_PROGRESS, COMPLETED, FAILED, CANCELLED)
- Progression (0-100%)
- Nombre de lignes traitÃ©es
- Messages d'erreur

## ğŸš€ Utilisation

### Recherche dans l'index local

```typescript
import { RNBIndexer } from '@/services/external-apis/rnb-indexer'

const indexer = new RNBIndexer()

// Recherche par code postal
const building = await indexer.searchBuilding(
  '75001',
  undefined,
  undefined
)

// Recherche par adresse
const building2 = await indexer.searchBuilding(
  undefined,
  '1 rue de la Paix, 75001 Paris',
  undefined
)

// Recherche par coordonnÃ©es
const building3 = await indexer.searchBuilding(
  undefined,
  undefined,
  { lat: 48.8566, lng: 2.3522 }
)
```

### Import progressif d'un dÃ©partement

```typescript
import { RNBImporter } from '@/services/external-apis/rnb-importer'

const importer = new RNBImporter()

const result = await importer.importDepartment({
  department: '75', // Paris
  maxRows: 10000, // Optionnel : limite pour test
  batchSize: 1000, // Taille des batches
  onProgress: (progress) => {
    console.log(`${progress.percentage}% - ${progress.processed}/${progress.total}`)
  },
})

console.log(`IndexÃ©: ${result.indexed}, Erreurs: ${result.errors}`)
```

### API Routes

#### POST `/api/rnb/import`
Lance l'import d'un dÃ©partement :
```json
{
  "department": "75",
  "maxRows": 10000
}
```

#### GET `/api/rnb/import`
RÃ©cupÃ¨re les jobs actifs et statistiques

#### POST `/api/rnb/search`
Recherche des bÃ¢timents :
```json
{
  "postalCode": "75001",
  "address": "1 rue de la Paix",
  "coordinates": { "lat": 48.8566, "lng": 2.3522 }
}
```

## ğŸ“Š Workflow d'Indexation

1. **Lancement** : Appel API `/api/rnb/import` avec dÃ©partement
2. **Job crÃ©Ã©** : Status PENDING dans `RNBImportJob`
3. **TÃ©lÃ©chargement** : Fichier ZIP depuis data.gouv.fr
4. **Parsing** : Extraction CSV ligne par ligne (streaming)
5. **Indexation** : Batch de 1000 bÃ¢timents dans `RNBBuilding`
6. **Progression** : Mise Ã  jour `progress` et `processedRows`
7. **Completion** : Status COMPLETED ou FAILED

## ğŸ”§ ImplÃ©mentation Technique

### Parsing CSV (Ã€ implÃ©menter)

Le parsing CSV nÃ©cessite une bibliothÃ¨que comme :
- `csv-parser` : Streaming CSV parser
- `papaparse` : CSV parser avec streaming
- `unzipper` : DÃ©compression ZIP stream

Exemple avec `csv-parser` :
```typescript
import csv from 'csv-parser'
import { createReadStream } from 'fs'
import { pipeline } from 'stream/promises'

const buildings: RNBBuildingData[] = []

await pipeline(
  createReadStream(csvFilePath),
  csv(),
  async function* (source) {
    for await (const row of source) {
      const building = parseCSVRow(row, department)
      if (building) {
        buildings.push(building)
        
        if (buildings.length >= batchSize) {
          await indexer.indexBuildingsBatch(buildings)
          buildings.length = 0
        }
      }
    }
  }
)
```

### Optimisations

- **Index PostgreSQL** : Sur `department`, `codeINSEE`, `postalCode`, `dpeClass`
- **Batch processing** : Traitement par lots de 1000
- **Streaming** : Pas de chargement complet en mÃ©moire
- **Transactions** : Commits par batch pour Ã©viter rollback complet

## ğŸ“ˆ Statistiques

RÃ©cupÃ©rer les statistiques d'indexation :
```typescript
const stats = await indexer.getIndexingStats()
// { "75": 12345, "92": 6789, ... }
```

## ğŸ¯ Prochaines Ã‰tapes

1. **ImplÃ©menter le parsing CSV** avec bibliothÃ¨que streaming
2. **Ajouter recherche gÃ©ographique** avec PostGIS (si disponible)
3. **CrÃ©er interface admin** pour monitorer les imports
4. **Ajouter filtres avancÃ©s** (DPEClass, HVD, annÃ©e construction)
5. **Optimiser requÃªtes** avec Full-Text Search PostgreSQL

## ğŸ“ Notes

- Les fichiers CSV sont trÃ¨s volumineux (50-200MB)
- L'import complet d'un dÃ©partement peut prendre plusieurs heures
- Recommandation : Importer progressivement les dÃ©partements les plus demandÃ©s
- Utiliser `maxRows` pour tester avant import complet

